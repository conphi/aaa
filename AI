###### srt trans rtx pro 5000 blackwell
watch -n 1 nvidia-smi
git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp
apt install nvcc
rm -rf build
cmake -B build -DGGML_CUDA=ON -DCMAKE_CUDA_ARCHITECTURES=90
cmake --build build --config Release -j$(nproc)

/root/llama/bin/llama-server -m /root/srt/Qwen2.5-72B-Instruct-abliterated.Q4_K_S.gguf -ngl 99 --port 8080 --host 0.0.0.0
uv run translate_srt.py 1408406-1_jp.srt

### onlyã€€trans translate_srt.py
import re
import time
import requests
import argparse
import os

# --- é…ç½® ---
API_URL = "http://127.0.0.1:8080/v1/chat/completions"

def parse_srt(file_path):
    """è§£æ SRT æ–‡ä»¶ï¼Œç¡®ä¿èƒ½å¤Ÿå¤„ç†éæ ‡å‡†æ¢è¡Œ"""
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read().strip()
    
    # å…¼å®¹ä¸åŒç³»ç»Ÿçš„æ¢è¡Œç¬¦å¹¶åˆ†å‰²å—
    blocks = re.split(r'\n\s*\n', content)
    parsed_subs = []
    
    for block in blocks:
        lines = block.strip().split('\n')
        if len(lines) >= 3:
            index = lines[0]
            timestamp = lines[1]
            # åˆå¹¶å¯èƒ½å­˜åœ¨çš„å¤šè¡Œæ­£æ–‡
            text = " ".join(lines[2:]).strip()
            if text:
                parsed_subs.append({
                    "index": index,
                    "timestamp": timestamp,
                    "text_ja": text,
                    "text_zh": ""
                })
    return parsed_subs

def translate_batch(batch):
    """é€šè¿‡ API æ‰§è¡Œç¿»è¯‘ä»»åŠ¡"""
    # æ„é€ å¸¦ç¼–å·çš„åˆ—è¡¨ï¼Œæ–¹ä¾¿æ¨¡å‹å¯¹é½
    input_text = "\n".join([f"{j+1}. {item['text_ja']}" for j, item in enumerate(batch)])
    
    payload = {
        "messages": [
            {
                "role": "system", 
                "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šç¿»è¯‘å®˜ã€‚ä»»åŠ¡ï¼šå°†æ—¥æ–‡é€è¡Œç¿»è¯‘æˆä¸­æ–‡ã€‚è¦æ±‚ï¼šåœ°é“ã€è‡ªç„¶ã€‚ç¦æ­¢é‡å¤åŸæ–‡ï¼Œç¦æ­¢è¾“å‡ºæ—¥æ–‡ï¼Œç¦æ­¢ä»»ä½•è§£é‡Šã€‚ä¸¥æ ¼ä¿æŒè¡Œå·æ ¼å¼ã€‚"
            },
            {
                "role": "user", 
                "content": f"è¯·ç¿»è¯‘ä»¥ä¸‹å†…å®¹ï¼š\n{input_text}"
            }
        ],
        "temperature": 0.1, # æä½æ¸©åº¦ä¿è¯è¾“å‡ºæ ¼å¼ç¨³å®š
        "max_tokens": 2048
    }

    try:
        response = requests.post(API_URL, json=payload, timeout=120)
        res_json = response.json()
        translated_content = res_json['choices'][0]['message']['content'].strip()
        
        # æå–æ¯ä¸€è¡Œ
        translated_lines = [l.strip() for l in translated_content.split('\n') if l.strip()]
        
        for k, line in enumerate(translated_lines):
            if k < len(batch):
                # æ­£åˆ™æ¸…ç†æ¨¡å‹å¯èƒ½è¾“å‡ºçš„ç¼–å·ï¼ˆå¦‚ 1. æˆ– 1ã€ï¼‰
                clean_line = re.sub(r'^\d+[\.ã€\s\-\:]+', '', line)
                batch[k]['text_zh'] = clean_line
            
    except Exception as e:
        print(f"\n[é”™è¯¯] ç¬¬ {batch[0]['index']} æ¡é™„è¿‘è¯·æ±‚å¼‚å¸¸: {e}")
        for item in batch:
            item['text_zh'] = "ï¼ˆç¿»è¯‘å‡ºé”™ï¼‰" + item['text_ja']

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("input", help="è¾“å…¥çš„æ—¥æ–‡ SRT æ–‡ä»¶è·¯å¾„")
    args = parser.parse_args()

    if not os.path.exists(args.input):
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {args.input}")
        return

    print(f"ğŸ“‚ æ­£åœ¨è§£ææ—¥æ–‡ SRT: {args.input}")
    subs = parse_srt(args.input)
    total = len(subs)
    
    # è®¾å®šæ‰¹æ¬¡å¤§å°ã€‚Qwen-72B åœ¨ Blackwell ä¸Šå¤„ç† 10 æ¡ä¸€æ‰¹éå¸¸è½»æ¾
    batch_size = 10 
    print(f"ğŸ§  å¼€å§‹ç¿»è¯‘ (æ€»è®¡ {total} æ¡)...")
    
    start_all = time.time()
    for i in range(0, total, batch_size):
        batch = subs[i : i + batch_size]
        translate_batch(batch)
        print(f"ğŸš€ è¿›åº¦: {min(i + batch_size, total)} / {total}", end='\r')

    # ç”Ÿæˆè¾“å‡ºè·¯å¾„
    output_file = args.input.replace(".srt", "_zh.srt")
    if output_file == args.input:
        output_file = "translated_output.srt"

    # å†™å…¥æ–°æ–‡ä»¶
    with open(output_file, 'w', encoding='utf-8') as f:
        for item in subs:
            # å¦‚æœç¿»è¯‘è½ç©ºï¼Œè‡³å°‘ä¿ç•™æ—¥æ–‡åŸæ–‡é˜²æ­¢å­—å¹•æ–­æ¡£
            final_text = item['text_zh'] if item['text_zh'] else item['text_ja']
            f.write(f"{item['index']}\n{item['timestamp']}\n{final_text}\n\n")
    
    print(f"\n\nâœ¨ ä»»åŠ¡å®Œæˆï¼è€—æ—¶: {time.time() - start_all:.2f}s")
    print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜è‡³: {output_file}")

if __name__ == "__main__":
    main()

############################################
proxychains uvx hf auth login
proxychains uvx hf download black-forest-labs/FLUX.2-klein-9b-fp8 flux-2-klein-9b-fp8.safetensors --local-dir .

### huggingface 
huggingface-cli -h

proxychains huggingface-cli download lllyasviel/misc --local-dir ./
proxychains huggingface-cli download --resume-download

### for using curl or wget to add ###
git config --global credential.helper store
#############################################
proxychains curl -C - -O -L https://huggingface.co/fuck/.../sth.safetensors
wget -c "https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_t2v_1.3B_fp16.safetensors?download=true"

proxychains curl -C - -L \
"https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true" \
-o umt5_xxl_fp8_e4m3fn_scaled.safetensors

########## comfyui ##############
pfy
proxychains python main.py --listen --port 9001
fy
python main.py --listen --port 9001

~/.bash_aliases
#################################

###### IPAdapter v1 vs v2 ################
https://www.reddit.com/r/comfyui/comments/1bov4xw/maintaining_legacy_ipadapter_nodes_alongside_new/

###################### uk server #################
########## aws æœåŠ¡å™¨ é™åˆ¶ ###################
fallocate -l 8G /swapfile
chmod 600 /swapfile
mkswap /swapfile
swapon /swapfile
free -h
###################################################
hf download Qwen/Qwen2.5-1.5B --local-dir ./Qwen2.5-1.5B --max-workers 1
###############################################################

